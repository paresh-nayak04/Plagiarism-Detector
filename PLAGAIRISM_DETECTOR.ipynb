{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTDsJsBXAIu4xMYOQQ8+98",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paresh-nayak04/Plagiarism-Detector/blob/main/PLAGAIRISM_DETECTOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd5Vv2QLHwXk",
        "outputId": "5aa05fe7-2d6f-4329-f294-5649203d0c25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.51.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.10.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.28.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.12/dist-packages (0.9)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2025-11-10 14:18:41.596 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.597 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.597 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.599 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.600 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.601 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.603 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.604 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.604 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.606 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.608 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.609 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.610 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.611 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.612 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.613 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.614 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.615 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.616 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.616 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.618 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.620 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.620 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.620 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.621 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.622 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-10 14:18:41.623 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit\n",
        "!pip install docx2txt\n",
        "!pip install PyPDF2\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import io\n",
        "import docx2txt\n",
        "from PyPDF2 import PdfReader\n",
        "import plotly.express as px\n",
        "\n",
        "def get_sentences(text):\n",
        "    sentences = tokenize.sent_tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "def get_url(sentence):\n",
        "    base_url = 'https://www.google.com/search?q='\n",
        "    query = sentence\n",
        "    query = query.replace(' ', '+')\n",
        "    url = base_url + query\n",
        "    headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}\n",
        "    res = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')\n",
        "    divs = soup.find_all('div', class_='yuRUbf')\n",
        "    urls = []\n",
        "    for div in divs:\n",
        "        a = div.find('a')\n",
        "        urls.append(a['href'])\n",
        "    if len(urls) == 0:\n",
        "        return None\n",
        "    elif \"youtube\" in urls[0]:\n",
        "        return None\n",
        "    else:\n",
        "        return urls[0]\n",
        "\n",
        "def read_text_file(file):\n",
        "    content = \"\"\n",
        "    with io.open(file.name, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "    return content\n",
        "\n",
        "def read_docx_file(file):\n",
        "    text = docx2txt.process(file)\n",
        "    return text\n",
        "\n",
        "def read_pdf_file(file):\n",
        "    text = \"\"\n",
        "    pdf_reader = PdfReader(file)\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def get_text_from_file(uploaded_file):\n",
        "    content = \"\"\n",
        "    if uploaded_file is not None:\n",
        "        if uploaded_file.type == \"text/plain\":\n",
        "            content = read_text_file(uploaded_file)\n",
        "        elif uploaded_file.type == \"application/pdf\":\n",
        "            content = read_pdf_file(uploaded_file)\n",
        "        elif uploaded_file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
        "            content = read_docx_file(uploaded_file)\n",
        "    return content\n",
        "\n",
        "def get_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))\n",
        "    return text\n",
        "\n",
        "def get_similarity(text1, text2):\n",
        "    text_list = [text1, text2]\n",
        "    cv = CountVectorizer()\n",
        "    count_matrix = cv.fit_transform(text_list)\n",
        "    similarity = cosine_similarity(count_matrix)[0][1]\n",
        "    return similarity\n",
        "\n",
        "def get_similarity_list(texts, filenames=None):\n",
        "    similarity_list = []\n",
        "    if filenames is None:\n",
        "        filenames = [f\"File {i+1}\" for i in range(len(texts))]\n",
        "    for i in range(len(texts)):\n",
        "        for j in range(i+1, len(texts)):\n",
        "            similarity = get_similarity(texts[i], texts[j])\n",
        "            similarity_list.append((filenames[i], filenames[j], similarity))\n",
        "    return similarity_list\n",
        "def get_similarity_list2(text, url_list):\n",
        "    similarity_list = []\n",
        "    for url in url_list:\n",
        "        text2 = get_text(url)\n",
        "        similarity = get_similarity(text, text2)\n",
        "        similarity_list.append(similarity)\n",
        "    return similarity_list\n",
        "\n",
        "def plot_scatter(df):\n",
        "    fig = px.scatter(df, x='File 1', y='File 2', color='Similarity', title='Similarity Scatter Plot')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def plot_line(df):\n",
        "    fig = px.line(df, x='File 1', y='File 2', color='Similarity', title='Similarity Line Chart')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def plot_bar(df):\n",
        "    fig = px.bar(df, x='File 1', y='Similarity', color='File 2', title='Similarity Bar Chart')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def plot_pie(df):\n",
        "    fig = px.pie(df, values='Similarity', names='File 1', title='Similarity Pie Chart')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def plot_box(df):\n",
        "    fig = px.box(df, x='File 1', y='Similarity', title='Similarity Box Plot')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def plot_histogram(df):\n",
        "    fig = px.histogram(df, x='Similarity', title='Similarity Histogram')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def plot_3d_scatter(df):\n",
        "    fig = px.scatter_3d(df, x='File 1', y='File 2', z='Similarity', color='Similarity',\n",
        "                        title='Similarity 3D Scatter Plot')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def plot_violin(df):\n",
        "    fig = px.violin(df, y='Similarity', x='File 1', title='Similarity Violin Plot')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "\n",
        "st.set_page_config(page_title='Plagiarism Detection')\n",
        "st.title('Plagiarism Detector')\n",
        "\n",
        "st.write(\"\"\"\n",
        "### Enter the text or upload a file to check for plagiarism or find similarities between files\n",
        "\"\"\")\n",
        "option = st.radio(\n",
        "    \"Select input option:\",\n",
        "    ('Enter text', 'Upload file', 'Find similarities between files')\n",
        ")\n",
        "\n",
        "if option == 'Enter text':\n",
        "    text = st.text_area(\"Enter text here\", height=200)\n",
        "    uploaded_files = []\n",
        "elif option == 'Upload file':\n",
        "    uploaded_file = st.file_uploader(\"Upload file (.docx, .pdf, .txt)\", type=[\"docx\", \"pdf\", \"txt\"])\n",
        "    if uploaded_file is not None:\n",
        "        text = get_text_from_file(uploaded_file)\n",
        "        uploaded_files = [uploaded_file]\n",
        "    else:\n",
        "        text = \"\"\n",
        "        uploaded_files = []\n",
        "else:\n",
        "    uploaded_files = st.file_uploader(\"Upload multiple files (.docx, .pdf, .txt)\", type=[\"docx\", \"pdf\", \"txt\"], accept_multiple_files=True)\n",
        "    texts = []\n",
        "    filenames = []\n",
        "    for uploaded_file in uploaded_files:\n",
        "        if uploaded_file is not None:\n",
        "            text = get_text_from_file(uploaded_file)\n",
        "            texts.append(text)\n",
        "            filenames.append(uploaded_file.name)\n",
        "    text = \" \".join(texts)\n",
        "\n",
        "if st.button('Check for plagiarism or find similarities'):\n",
        "    st.write(\"\"\"\n",
        "    ### Checking for plagiarism or finding similarities...\n",
        "    \"\"\")\n",
        "    if not text:\n",
        "        st.write(\"\"\"\n",
        "        ### No text found for plagiarism check or finding similarities.\n",
        "        \"\"\")\n",
        "        st.stop()\n",
        "\n",
        "    if option == 'Find similarities between files':\n",
        "        similarities = get_similarity_list(texts, filenames)\n",
        "        df = pd.DataFrame(similarities, columns=['File 1', 'File 2', 'Similarity'])\n",
        "        df = df.sort_values(by=['Similarity'], ascending=False)\n",
        "        # Plotting interactive graphs\n",
        "        plot_scatter(df)\n",
        "        plot_line(df)\n",
        "        plot_bar(df)\n",
        "        plot_pie(df)\n",
        "        plot_box(df)\n",
        "        plot_histogram(df)\n",
        "        plot_3d_scatter(df)\n",
        "        plot_violin(df)\n",
        "    else:\n",
        "        sentences = get_sentences(text)\n",
        "        url = []\n",
        "        for sentence in sentences:\n",
        "            url.append(get_url(sentence))\n",
        "\n",
        "        if None in url:\n",
        "            st.write(\"\"\"\n",
        "            ### No plagiarism detected!\n",
        "            \"\"\")\n",
        "            st.stop()\n",
        "\n",
        "        similarity_list = get_similarity_list2(text, url)\n",
        "        df = pd.DataFrame({'Sentence': sentences, 'URL': url, 'Similarity': similarity_list})\n",
        "        df = df.sort_values(by=['Similarity'], ascending=True)\n",
        "\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # Make URLs clickable in the DataFrame\n",
        "    if 'URL' in df.columns:\n",
        "        df['URL'] = df['URL'].apply(lambda x: '<a href=\"{}\">{}</a>'.format(x, x) if x else '')\n",
        "\n",
        "    # Center align URL column header\n",
        "    df_html = df.to_html(escape=False)\n",
        "    if 'URL' in df.columns:\n",
        "        df_html = df_html.replace('<th>URL</th>', '<th style=\"text-align: center;\">URL</th>')\n",
        "    st.write(df_html, unsafe_allow_html=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dec8ff9b"
      },
      "source": [
        "To run your Streamlit application, you need to:\n",
        "\n",
        "1. Save the code from the previous cell into a Python file (e.g., `app.py`).\n",
        "2. Run the Streamlit application using the `streamlit run` command, along with `npx localtunnel` to expose the app publicly.\n",
        "\n",
        "Here's how you can do it:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6891d7b",
        "outputId": "54d2b571-7294-494e-a853-92ccaa2cb18a"
      },
      "source": [
        "%%writefile app.py\n",
        "# This cell writes the content of the previous code cell to a file named 'app.py'\n",
        "# Please ensure the previous cell contains the complete Streamlit application code\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt') # Ensure 'punkt' is downloaded\n",
        "nltk.download('punkt_tab') # Download 'punkt_tab' for the tokenizer\n",
        "from nltk import tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import io\n",
        "import docx2txt\n",
        "from PyPDF2 import PdfReader\n",
        "import plotly.express as px\n",
        "\n",
        "def get_sentences(text):\n",
        "    sentences = tokenize.sent_tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "def get_url(sentence):\n",
        "    base_url = 'https://www.google.com/search?q='\n",
        "    query = sentence\n",
        "    query = query.replace(' ', '+')\n",
        "    url = base_url + query\n",
        "    headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}\n",
        "    res = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')\n",
        "    divs = soup.find_all('div', class_='yuRUbf')\n",
        "    urls = []\n",
        "    for div in divs:\n",
        "        a = div.find('a')\n",
        "        urls.append(a['href'])\n",
        "    if len(urls) == 0:\n",
        "        return None\n",
        "    elif \"youtube\" in urls[0]:\n",
        "        return None\n",
        "    else:\n",
        "        return urls[0]\n",
        "\n",
        "def read_text_file(file):\n",
        "    # Streamlit's uploaded file is a BytesIO-like object\n",
        "    content = file.getvalue().decode('utf-8')\n",
        "    return content\n",
        "\n",
        "def read_docx_file(file):\n",
        "    # docx2txt can process file-like objects directly\n",
        "    text = docx2txt.process(file)\n",
        "    return text\n",
        "\n",
        "def read_pdf_file(file):\n",
        "    text = \"\"\n",
        "    # PyPDF2 PdfReader can process file-like objects directly\n",
        "    pdf_reader = PdfReader(file)\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def get_text_from_file(uploaded_file):\n",
        "    content = \"\"\n",
        "    if uploaded_file is not None:\n",
        "        if uploaded_file.type == \"text/plain\":\n",
        "            content = read_text_file(uploaded_file)\n",
        "        elif uploaded_file.type == \"application/pdf\":\n",
        "            content = read_pdf_file(uploaded_file)\n",
        "        elif uploaded_file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
        "            content = read_docx_file(uploaded_file)\n",
        "    return content\n",
        "\n",
        "def get_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))\n",
        "    return text\n",
        "\n",
        "def get_similarity(text1, text2):\n",
        "    text_list = [text1, text2]\n",
        "    cv = CountVectorizer()\n",
        "    count_matrix = cv.fit_transform(text_list)\n",
        "    similarity = cosine_similarity(count_matrix)[0][1]\n",
        "    return similarity\n",
        "\n",
        "def get_similarity_list(texts, filenames=None):\n",
        "    similarity_list = []\n",
        "    if filenames is None:\n",
        "        filenames = [f\"File {i+1}\" for i in range(len(texts))]\n",
        "    for i in range(len(texts)):\n",
        "        for j in range(i+1, len(texts)):\n",
        "            similarity = get_similarity(texts[i], texts[j])\n",
        "            similarity_list.append((filenames[i], filenames[j], similarity))\n",
        "    return similarity_list\n",
        "def get_similarity_list2(text, url_list):\n",
        "    similarity_list = []\n",
        "    for url in url_list:\n",
        "        text2 = get_text(url)\n",
        "        similarity = get_similarity(text, text2)\n",
        "        similarity_list.append(similarity)\n",
        "    return similarity_list\n",
        "\n",
        "def plot_scatter(df):\n",
        "    fig = px.scatter(df, x='File 1', y='File 2', color='Similarity', title='Similarity Scatter Plot')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def plot_line(df):\n",
        "    fig = px.line(df, x='File 1', y='File 2', color='Similarity', title='Similarity Line Chart')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def plot_bar(df):\n",
        "    fig = px.bar(df, x='File 1', y='Similarity', color='File 2', title='Similarity Bar Chart')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def plot_pie(df):\n",
        "    fig = px.pie(df, values='Similarity', names='File 1', title='Similarity Pie Chart')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def plot_box(df):\n",
        "    fig = px.box(df, x='File 1', y='Similarity', title='Similarity Box Plot')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def plot_histogram(df):\n",
        "    fig = px.histogram(df, x='Similarity', title='Similarity Histogram')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def plot_3d_scatter(df):\n",
        "    fig = px.scatter_3d(df, x='File 1', y='File 2', z='Similarity', color='Similarity',\n",
        "                        title='Similarity 3D Scatter Plot')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def plot_violin(df):\n",
        "    fig = px.violin(df, y='Similarity', x='File 1', title='Similarity Violin Plot')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "\n",
        "st.set_page_config(page_title='Plagiarism Detection')\n",
        "st.title('Plagiarism Detector')\n",
        "\n",
        "st.write(\"\"\"\n",
        "### Enter the text or upload a file to check for plagiarism or find similarities between files\n",
        "\"\"\")\n",
        "option = st.radio(\n",
        "    \"Select input option:\",\n",
        "    ('Enter text', 'Upload file', 'Find similarities between files')\n",
        ")\n",
        "\n",
        "if option == 'Enter text':\n",
        "    text = st.text_area(\"Enter text here\", height=200)\n",
        "    uploaded_files = []\n",
        "elif option == 'Upload file':\n",
        "    uploaded_file = st.file_uploader(\"Upload file (.docx, .pdf, .txt)\", type=[\"docx\", \"pdf\", \"txt\"])\n",
        "    if uploaded_file is not None:\n",
        "        text = get_text_from_file(uploaded_file)\n",
        "        uploaded_files = [uploaded_file]\n",
        "    else:\n",
        "        text = \"\"\n",
        "        uploaded_files = []\n",
        "else:\n",
        "    uploaded_files = st.file_uploader(\"Upload multiple files (.docx, .pdf, .txt)\", type=[\"docx\", \"pdf\", \"txt\"], accept_multiple_files=True)\n",
        "    texts = []\n",
        "    filenames = []\n",
        "    for uploaded_file in uploaded_files:\n",
        "        if uploaded_file is not None:\n",
        "            text = get_text_from_file(uploaded_file)\n",
        "            texts.append(text)\n",
        "            filenames.append(uploaded_file.name)\n",
        "    text = \" \".join(texts)\n",
        "\n",
        "if st.button('Check for plagiarism or find similarities'):\n",
        "    st.write(\"\"\"\n",
        "    ### Checking for plagiarism or finding similarities...\n",
        "    \"\"\")\n",
        "    if not text:\n",
        "        st.write(\"\"\"\n",
        "        ### No text found for plagiarism check or finding similarities.\n",
        "        \"\"\")\n",
        "        st.stop()\n",
        "\n",
        "    if option == 'Find similarities between files':\n",
        "        similarities = get_similarity_list(texts, filenames)\n",
        "        df = pd.DataFrame(similarities, columns=['File 1', 'File 2', 'Similarity'])\n",
        "        df = df.sort_values(by=['Similarity'], ascending=False)\n",
        "        # Plotting interactive graphs\n",
        "        plot_scatter(df)\n",
        "        plot_line(df)\n",
        "        plot_bar(df)\n",
        "        plot_pie(df)\n",
        "        plot_box(df)\n",
        "        plot_histogram(df)\n",
        "        plot_3d_scatter(df)\n",
        "        plot_violin(df)\n",
        "    else: # Plagiarism check for single input text/file\n",
        "        sentences = get_sentences(text)\n",
        "        if not sentences:\n",
        "            st.write(\"### No sentences found in the provided text for plagiarism check.\")\n",
        "            st.stop()\n",
        "\n",
        "        results_data = []\n",
        "        for sentence in sentences:\n",
        "            online_url = get_url(sentence) # Get the most relevant URL for the sentence\n",
        "            if online_url:\n",
        "                try:\n",
        "                    # Fetch content from the URL\n",
        "                    online_text = get_text(online_url)\n",
        "                    # Calculate similarity between the input sentence and the online content\n",
        "                    similarity_score = get_similarity(sentence, online_text)\n",
        "                    results_data.append({'Sentence': sentence, 'URL': online_url, 'Similarity': similarity_score})\n",
        "                except requests.exceptions.RequestException:\n",
        "                    # Handle cases where fetching the URL content fails (e.g., connection error, 404)\n",
        "                    results_data.append({'Sentence': sentence, 'URL': online_url + \" (Error fetching content)\", 'Similarity': 0.0})\n",
        "                except Exception as e:\n",
        "                    # Catch any other unexpected errors during processing\n",
        "                    print(f\"An unexpected error occurred for sentence: '{sentence}' and URL: {online_url} - {e}\")\n",
        "                    results_data.append({'Sentence': sentence, 'URL': online_url + \" (Processing error)\", 'Similarity': 0.0})\n",
        "            else:\n",
        "                # If no relevant online URL was found for the sentence\n",
        "                results_data.append({'Sentence': sentence, 'URL': 'No direct online match found', 'Similarity': 0.0})\n",
        "\n",
        "        if not results_data:\n",
        "            st.write(\"### Could not process any sentences for plagiarism check.\")\n",
        "            st.stop()\n",
        "\n",
        "        df = pd.DataFrame(results_data)\n",
        "        # Sort by similarity in descending order to show highest matches first\n",
        "        df = df.sort_values(by=['Similarity'], ascending=False)\n",
        "\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # Make URLs clickable in the DataFrame\n",
        "    if 'URL' in df.columns:\n",
        "        df['URL'] = df['URL'].apply(lambda x: '<a href=\"{}\">{}</a>'.format(x, x) if x else '')\n",
        "\n",
        "    # Center align URL column header\n",
        "    df_html = df.to_html(escape=False)\n",
        "    if 'URL' in df.columns:\n",
        "        df_html = df_html.replace('<th>URL</th>', '<th style=\"text-align: center;\">URL</th>')\n",
        "    st.write(df_html, unsafe_allow_html=True)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok docx2txt PyPDF2 plotly scikit-learn beautifulsoup4 nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxGbdtffMV9u",
        "outputId": "8aeecf32-8585-46fb-b0eb-c0b5cdf78ae6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.51.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.4.1-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.12/dist-packages (0.9)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.10.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.28.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading pyngrok-7.4.1-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill any existing tunnels (clean start)\n",
        "ngrok.kill()\n",
        "\n",
        "# !! IMPORTANT: Replace 'YOUR_AUTHTOKEN' with your actual ngrok authtoken !!\n",
        "# You can get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "ngrok.set_auth_token(\"35I78QR6zTiSH9DcWGGu65enOh1_G1NW859WXgp2iPPwQvgd\")\n",
        "\n",
        "# Start a new tunnel on port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"ðŸŒ Your Streamlit app will be live here:\\n{public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xad0c5HfMWok",
        "outputId": "36dcebbf-fa8a-4391-e909-9355be60ef64"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŒ Your Streamlit app will be live here:\n",
            "NgrokTunnel: \"https://crosby-neighborly-charity.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef702b5e"
      },
      "source": [
        "### Debugging `get_url` function\n",
        "Let's test the `get_url` function with a sample sentence from Wikipedia to see if it correctly extracts a URL. The most common reason for this issue is that Google's HTML structure for search results has changed, making the `yuRUbf` class selector outdated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28eef06a",
        "outputId": "79c00c3f-c4f1-4492-e51b-9abab991c5b9"
      },
      "source": [
        "sample_wikipedia_sentence = \"Wikipedia is a multilingual free online encyclopedia written and maintained by a community of volunteers through open collaboration and a wiki-based editing system.\"\n",
        "\n",
        "# Re-define get_url (copying from app.py to ensure we're testing the exact logic)\n",
        "def get_url(sentence):\n",
        "    base_url = 'https://www.google.com/search?q='\n",
        "    query = sentence\n",
        "    query = query.replace(' ', '+')\n",
        "    url = base_url + query\n",
        "    headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}\n",
        "    res = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')\n",
        "\n",
        "    print(f\"HTTP Status Code: {res.status_code}\")\n",
        "    print(f\"First 500 chars of response: {res.text[:500]}\")\n",
        "\n",
        "    urls = []\n",
        "\n",
        "    # Try to find common Google search result links\n",
        "    # Look for a tags that are direct descendants of div elements with specific roles or data attributes\n",
        "    # Or links that start with /url?q=\n",
        "    for a_tag in soup.find_all('a', href=True):\n",
        "        href = a_tag['href']\n",
        "        if href.startswith('/url?q=') and not href.startswith('/url?q=/search'):\n",
        "            # Extract the clean URL from Google's redirect link\n",
        "            clean_url = href.split('/url?q=')[1].split('&sa=U')[0]\n",
        "            urls.append(clean_url)\n",
        "        elif 'wikipedia.org' in href and not href.startswith('/') and 'google.com' not in href:\n",
        "            # A fallback for direct Wikipedia links if the /url?q= pattern fails or is not primary\n",
        "            urls.append(href)\n",
        "        # For debugging, print all found hrefs\n",
        "        print(f\"Found href: {href}\")\n",
        "\n",
        "    if not urls:\n",
        "        # Fallback to broader search, looking for links within common result blocks\n",
        "        # Often, main results are within div tags with specific classes or roles\n",
        "        for div in soup.find_all('div', class_=['g', 'rc', 'yuRUbf', 'tF2CMy']):\n",
        "            a = div.find('a', href=True)\n",
        "            if a and not a['href'].startswith('/search') and 'google.com' not in a['href']:\n",
        "                urls.append(a['href'])\n",
        "\n",
        "    # Filter out YouTube links and return the first non-YouTube URL\n",
        "    filtered_urls = [u for u in urls if \"youtube.com\" not in u and \"support.google.com\" not in u and u.startswith('http')]\n",
        "\n",
        "    if len(filtered_urls) == 0:\n",
        "        return None\n",
        "    else:\n",
        "        return filtered_urls[0]\n",
        "\n",
        "found_url = get_url(sample_wikipedia_sentence)\n",
        "print(f\"Final Found URL: {found_url}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTTP Status Code: 200\n",
            "First 500 chars of response: <!DOCTYPE html><html lang=\"en\"><head><title>Google Search</title><style>body{background-color:var(--xhUGwc)}</style><script nonce=\"7XzTKZpUti011DV2uqnasA\">window.google = window.google || {};window.google.c = window.google.c || {cap:0};</script></head><body><noscript><style>table,div,span,p{display:none}</style><meta content=\"0;url=/httpservice/retry/enablejs?sei=nf4RabK-KuPdwN4PxITd2Aw\" http-equiv=\"refresh\"><div style=\"display:block\">Please click <a href=\"/httpservice/retry/enablejs?sei=nf4RabK\n",
            "Found href: /httpservice/retry/enablejs?sei=nf4RabK-KuPdwN4PxITd2Aw\n",
            "Found href: /search?q=Wikipedia+is+a+multilingual+free+online+encyclopedia+written+and+maintained+by+a+community+of+volunteers+through+open+collaboration+and+a+wiki-based+editing+system.&sca_esv=3f626f5782a50780&emsg=SG_REL&sei=nf4RabK-KuPdwN4PxITd2Aw\n",
            "Found href: https://support.google.com/websearch\n",
            "Final Found URL: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port 8501 &>/dev/null&"
      ],
      "metadata": {
        "id": "AzWw-WS5MbQW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gkAWIzGgNyjL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}